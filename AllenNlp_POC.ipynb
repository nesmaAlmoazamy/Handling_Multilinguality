{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from overrides import overrides\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "# @DatasetReader.register(\"data-reader\")\n",
    "class MultilingualDatasetReader(DatasetReader):\n",
    "    def __init__(self,    \n",
    "        lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy = lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer(JustSpacesWordSplitter())\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=',')\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 2:\n",
    "                    Instance = self.text_to_instance( article=row[1],label=row[0])\n",
    "                    yield Instance\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,  # type: ignore\n",
    "\t\t                 article: str,\n",
    "\t\t                 label: str = None) -> Instance:\n",
    "\t\t# pylint: disable=arguments-differ\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_article = self._tokenizer.tokenize(article)\n",
    "        fields[\"tokens\"] = TextField(tokenized_article, self._token_indexers)\n",
    "#        fields[\"tokens\"] = article\n",
    "        if label is not None:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3601it [00:00, 7135.48it/s]\n",
      "902it [00:00, 4572.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# from realworldnlp.DatasetReader import MultilingualDatasetReader\n",
    "reader = MultilingualDatasetReader()\n",
    "train_dataset = reader.read('/home/nesma/SemesterII/Neural Networks/Project/multilingual-text-categorization-dataset/realworldnlp/train.csv')\n",
    "dev_dataset = reader.read('/home/nesma/SemesterII/Neural Networks/Project/multilingual-text-categorization-dataset/realworldnlp/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4503/4503 [00:00<00:00, 16509.34it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "            output[\"acc\"] = self.accuracy(logits, label)\n",
    "\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {'accuracy': self.accuracy.get_metric(reset)}\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LstmClassifier(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (encoder): PytorchSeq2VecWrapper(\n",
       "    (_module): LSTM(128, 128, batch_first=True)\n",
       "  )\n",
       "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
       "  (loss_function): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 1e-05\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.iterators.bucket_iterator.BucketIterator at 0x7f543442fb70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.regularizers.regularizers import L1Regularizer\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0436, loss: 3.7939 ||: 100%|██████████| 113/113 [00:14<00:00,  7.91it/s]\n",
      "accuracy: 0.0355, loss: 3.7758 ||: 100%|██████████| 29/29 [00:00<00:00, 40.03it/s]\n",
      "accuracy: 0.0780, loss: 3.5955 ||: 100%|██████████| 113/113 [00:12<00:00,  6.71it/s]\n",
      "accuracy: 0.0787, loss: 3.5371 ||: 100%|██████████| 29/29 [00:01<00:00, 28.90it/s]\n",
      "accuracy: 0.1155, loss: 3.3168 ||: 100%|██████████| 113/113 [00:12<00:00,  9.12it/s]\n",
      "accuracy: 0.0998, loss: 3.4359 ||: 100%|██████████| 29/29 [00:00<00:00, 43.50it/s]\n",
      "accuracy: 0.1727, loss: 2.9950 ||: 100%|██████████| 113/113 [00:13<00:00,  8.17it/s]\n",
      "accuracy: 0.1452, loss: 3.2763 ||: 100%|██████████| 29/29 [00:00<00:00, 31.87it/s]\n",
      "accuracy: 0.2560, loss: 2.5645 ||: 100%|██████████| 113/113 [00:13<00:00,  8.89it/s]\n",
      "accuracy: 0.1818, loss: 3.2382 ||: 100%|██████████| 29/29 [00:00<00:00, 41.58it/s]\n",
      "accuracy: 0.3852, loss: 2.1183 ||: 100%|██████████| 113/113 [00:11<00:00,  9.91it/s]\n",
      "accuracy: 0.2040, loss: 3.2386 ||: 100%|██████████| 29/29 [00:00<00:00, 42.68it/s]\n",
      "accuracy: 0.4946, loss: 1.7036 ||: 100%|██████████| 113/113 [00:10<00:00, 10.95it/s]\n",
      "accuracy: 0.2018, loss: 3.4089 ||: 100%|██████████| 29/29 [00:00<00:00, 44.89it/s]\n",
      "accuracy: 0.6251, loss: 1.2804 ||: 100%|██████████| 113/113 [00:10<00:00, 10.68it/s]\n",
      "accuracy: 0.2173, loss: 3.4811 ||: 100%|██████████| 29/29 [00:00<00:00, 45.43it/s]\n",
      "accuracy: 0.7156, loss: 0.9578 ||: 100%|██████████| 113/113 [00:10<00:00, 11.36it/s]\n",
      "accuracy: 0.2029, loss: 3.7261 ||: 100%|██████████| 29/29 [00:00<00:00, 43.89it/s]\n",
      "accuracy: 0.7734, loss: 0.7406 ||: 100%|██████████| 113/113 [00:11<00:00,  9.68it/s]\n",
      "accuracy: 0.2162, loss: 3.8702 ||: 100%|██████████| 29/29 [00:00<00:00, 35.33it/s]\n",
      "accuracy: 0.8184, loss: 0.5714 ||: 100%|██████████| 113/113 [00:13<00:00,  8.13it/s]\n",
      "accuracy: 0.2206, loss: 4.1186 ||: 100%|██████████| 29/29 [00:00<00:00, 40.30it/s]\n",
      "accuracy: 0.8564, loss: 0.4338 ||: 100%|██████████| 113/113 [00:12<00:00,  6.36it/s]\n",
      "accuracy: 0.2040, loss: 4.3159 ||: 100%|██████████| 29/29 [00:01<00:00, 28.31it/s]\n",
      "accuracy: 0.8656, loss: 0.3690 ||: 100%|██████████| 113/113 [00:14<00:00,  8.75it/s]\n",
      "accuracy: 0.2129, loss: 4.3125 ||: 100%|██████████| 29/29 [00:00<00:00, 29.48it/s]\n",
      "accuracy: 0.8778, loss: 0.3092 ||: 100%|██████████| 113/113 [00:14<00:00,  9.31it/s]\n",
      "accuracy: 0.2051, loss: 4.4729 ||: 100%|██████████| 29/29 [00:00<00:00, 32.97it/s]\n",
      "accuracy: 0.8803, loss: 0.2891 ||: 100%|██████████| 113/113 [00:11<00:00,  8.95it/s]\n",
      "accuracy: 0.1973, loss: 4.6444 ||: 100%|██████████| 29/29 [00:00<00:00, 34.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'peak_cpu_memory_MB': 388.492,\n",
       " 'training_duration': '00:03:07',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 13,\n",
       " 'epoch': 13,\n",
       " 'training_accuracy': 0.8778117189669536,\n",
       " 'training_loss': 0.3091622807283317,\n",
       " 'training_cpu_memory_MB': 388.492,\n",
       " 'validation_accuracy': 0.20509977827050999,\n",
       " 'validation_loss': 4.472880568997613,\n",
       " 'best_validation_accuracy': 0.18181818181818182,\n",
       " 'best_validation_loss': 3.2382413683266473}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "\n",
    "# @Predictor.register('text_classifier')\n",
    "class TextClassifierPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor for any model that takes in a sentence and returns\n",
    "    a single class for it.  In particular, it can be used with\n",
    "    the :class:`~allennlp.models.basic_classifier.BasicClassifier` model\n",
    "    \"\"\"\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like ``{\"sentence\": \"...\"}``.\n",
    "        Runs the underlying model, and adds the ``\"label\"`` to the output.\n",
    "        \"\"\"\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hobbies_and_interests\n"
     ]
    }
   ],
   "source": [
    "tokens = 'available in itunes about bad wolves a los angeles californiabased heavy metal outfit with an impressive hard rock'\n",
    "predictor = TextClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(tokens)['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing\n"
     ]
    }
   ],
   "source": [
    "tokens = 'jeans day julia and emma roberts sporting denim both emma and julia are rocking in their denim'\n",
    "predictor = TextClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(tokens)['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
