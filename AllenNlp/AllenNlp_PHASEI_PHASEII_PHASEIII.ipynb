{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase I:  Classification for each language, each one has a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe for all language from our original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "random.seed(30)\n",
    "dataset = pd.read_csv('/home/nesma/SemesterII/Neural Networks/Project/multilingual-text-categorization-dataset/data/dataset.csv', sep='\\t', header=None).applymap(str)\n",
    "dataset.columns = [\"language\",\"label\",\"text\"]\n",
    "languagesData=[]\n",
    "loc = 0\n",
    "languages = dataset[dataset.columns[0]].unique()\n",
    "for i in languages:\n",
    "    name = languages[loc]+\"Data\" \n",
    "    globals()[name] = pd.DataFrame( dataset[dataset.language == i])\n",
    "    loc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some dataPrepocessing for all the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "def preprocessing(text):\n",
    "    text = text.str.lower()                                              #Lower\n",
    "    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])     #Remove Secured URL\n",
    "    text = text.apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])      #Remove URL\n",
    "    text = text.str.replace('\\d+', '')                                   #Remove_numbers\n",
    "    text = text.str.replace('[^\\w\\s]','')                                #Remove_punctuations\n",
    "    text = text.str.strip()                                              #remove_blank_space\n",
    "    text = text.replace('\\s+', ' ', regex=True)                          \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AllenNlp Classes for Dataset Reading, The model, And the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "from typing import Dict\n",
    "import logging\n",
    "import csv\n",
    "from overrides import overrides\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "# @DatasetReader.register(\"data-reader\")\n",
    "class MultilingualDatasetReader(DatasetReader):\n",
    "    def __init__(self,    \n",
    "        lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy = lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer(JustSpacesWordSplitter())\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=',')\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 2:\n",
    "                    Instance = self.text_to_instance( article=row[1],label=row[0])\n",
    "                    yield Instance\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,  # type: ignore\n",
    "                 article: str,\n",
    "                 label: str = None) -> Instance:\n",
    "        # pylint: disable=arguments-differ\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_article = self._tokenizer.tokenize(article)\n",
    "        fields[\"tokens\"] = TextField(tokenized_article, self._token_indexers)\n",
    "#        fields[\"tokens\"] = article\n",
    "        if label is not None:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "    \n",
    "    \n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "            output[\"acc\"] = self.accuracy(logits, label)\n",
    "\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {'accuracy': self.accuracy.get_metric(reset)}\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "\n",
    "# @Predictor.register('text_classifier')\n",
    "class TextClassifierPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor for any model that takes in a sentence and returns\n",
    "    a single class for it.  In particular, it can be used with\n",
    "    the :class:`~allennlp.models.basic_classifier.BasicClassifier` model\n",
    "    \"\"\"\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation, Including spliting the dataset for train and test, Also save those datasets to be reeded using the AllenNlp Reader lateron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds,lang):\n",
    "    ds['text'] = preprocessing(ds['text'])\n",
    "    ds = ds[[\"label\",\"text\"]]\n",
    "    df = pd.DataFrame(ds[\"label\"]) \n",
    "    df[\"text\"] = ds['text'].str.split().str[1:100].str.join(\" \")\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    train_dataset = df[msk]\n",
    "    test_dataset = df[~msk]\n",
    "    train_dataset.to_csv(str(lang)+\"train.csv\",index=False,header=False)\n",
    "    test_dataset.to_csv(str(lang)+\"test.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLang(DS,lang):\n",
    "    prepare(DS,lang)\n",
    "    reader = MultilingualDatasetReader()\n",
    "    train_dataset = reader.read(str(lang) + 'train.csv')\n",
    "    dev_dataset = reader.read(str(lang)+'test.csv')\n",
    "\n",
    "    vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                      min_count={'tokens': 3})\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=EMBEDDING_DIM)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "    encoder = PytorchSeq2VecWrapper(\n",
    "        torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "    model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "    print(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    print(optimizer)\n",
    "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=dev_dataset,\n",
    "                      patience=10,\n",
    "                      num_epochs=50)\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3594it [00:01, 3505.70it/s]\n",
      "925it [00:00, 7123.64it/s]\n",
      "100%|██████████| 4519/4519 [00:00<00:00, 13557.34it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0434, loss: 3.7941 ||: 100%|██████████| 113/113 [00:24<00:00,  4.23it/s]\n",
      "accuracy: 0.0843, loss: 3.7487 ||: 100%|██████████| 29/29 [00:01<00:00, 22.97it/s]\n",
      "accuracy: 0.0879, loss: 3.5959 ||: 100%|██████████| 113/113 [00:20<00:00,  5.00it/s]\n",
      "accuracy: 0.0930, loss: 3.4621 ||: 100%|██████████| 29/29 [00:01<00:00, 26.11it/s]\n",
      "accuracy: 0.1249, loss: 3.2155 ||: 100%|██████████| 113/113 [00:20<00:00,  5.67it/s]\n",
      "accuracy: 0.1049, loss: 3.3729 ||: 100%|██████████| 29/29 [00:01<00:00, 26.41it/s]\n",
      "accuracy: 0.1945, loss: 2.8350 ||: 100%|██████████| 113/113 [00:17<00:00,  7.50it/s]\n",
      "accuracy: 0.1330, loss: 3.3942 ||: 100%|██████████| 29/29 [00:00<00:00, 43.67it/s]\n",
      "accuracy: 0.2679, loss: 2.4563 ||: 100%|██████████| 113/113 [00:15<00:00,  6.88it/s]\n",
      "accuracy: 0.1557, loss: 3.4278 ||: 100%|██████████| 29/29 [00:00<00:00, 42.94it/s]\n",
      "accuracy: 0.3497, loss: 2.1849 ||: 100%|██████████| 113/113 [00:15<00:00,  6.65it/s]\n",
      "accuracy: 0.1697, loss: 3.5414 ||: 100%|██████████| 29/29 [00:00<00:00, 41.12it/s]\n",
      "accuracy: 0.4363, loss: 1.8376 ||: 100%|██████████| 113/113 [00:15<00:00,  7.20it/s]\n",
      "accuracy: 0.1914, loss: 3.6101 ||: 100%|██████████| 29/29 [00:00<00:00, 38.52it/s]\n",
      "accuracy: 0.5601, loss: 1.4451 ||: 100%|██████████| 113/113 [00:20<00:00,  5.58it/s]\n",
      "accuracy: 0.1730, loss: 3.9107 ||: 100%|██████████| 29/29 [00:00<00:00, 33.39it/s]\n",
      "accuracy: 0.6575, loss: 1.1363 ||: 100%|██████████| 113/113 [00:19<00:00,  7.04it/s]\n",
      "accuracy: 0.1751, loss: 4.1048 ||: 100%|██████████| 29/29 [00:00<00:00, 41.26it/s]\n",
      "accuracy: 0.7262, loss: 0.9057 ||: 100%|██████████| 113/113 [00:15<00:00,  6.94it/s]\n",
      "accuracy: 0.1784, loss: 4.2997 ||: 100%|██████████| 29/29 [00:00<00:00, 41.88it/s]\n",
      "accuracy: 0.7760, loss: 0.7175 ||: 100%|██████████| 113/113 [00:16<00:00,  6.82it/s]\n",
      "accuracy: 0.1795, loss: 4.4012 ||: 100%|██████████| 29/29 [00:00<00:00, 40.24it/s]\n",
      "accuracy: 0.8091, loss: 0.5795 ||: 100%|██████████| 113/113 [00:17<00:00,  7.44it/s]\n",
      "accuracy: 0.1827, loss: 4.6425 ||: 100%|██████████| 29/29 [00:00<00:00, 40.42it/s]\n",
      "accuracy: 0.8450, loss: 0.4726 ||: 100%|██████████| 113/113 [00:17<00:00,  6.84it/s]\n",
      "accuracy: 0.1849, loss: 4.7496 ||: 100%|██████████| 29/29 [00:00<00:00, 30.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 2, 'peak_cpu_memory_MB': 790.72, 'training_duration': '00:03:49', 'training_start_epoch': 0, 'training_epochs': 11, 'epoch': 11, 'training_accuracy': 0.8091263216471898, 'training_loss': 0.5794810180642963, 'training_cpu_memory_MB': 790.72, 'validation_accuracy': 0.1827027027027027, 'validation_loss': 4.642499874378073, 'best_validation_accuracy': 0.10486486486486486, 'best_validation_loss': 3.372904399345661}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainLang(englishData,\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3644it [00:01, 2483.97it/s]\n",
      "875it [00:00, 7443.04it/s]\n",
      "100%|██████████| 4519/4519 [00:00<00:00, 10867.81it/s]\n",
      "  0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0467, loss: 3.7929 ||: 100%|██████████| 114/114 [00:27<00:00,  4.09it/s]\n",
      "accuracy: 0.0640, loss: 3.7530 ||: 100%|██████████| 28/28 [00:01<00:00, 21.21it/s]\n",
      "accuracy: 0.0735, loss: 3.5898 ||: 100%|██████████| 114/114 [00:25<00:00,  4.30it/s]\n",
      "accuracy: 0.0811, loss: 3.5179 ||: 100%|██████████| 28/28 [00:01<00:00, 21.85it/s]\n",
      "accuracy: 0.1142, loss: 3.2544 ||: 100%|██████████| 114/114 [00:25<00:00,  5.12it/s]\n",
      "accuracy: 0.1177, loss: 3.4445 ||: 100%|██████████| 28/28 [00:01<00:00, 24.25it/s]\n",
      "accuracy: 0.1745, loss: 2.8891 ||: 100%|██████████| 114/114 [00:26<00:00,  4.30it/s]\n",
      "accuracy: 0.1383, loss: 3.4180 ||: 100%|██████████| 28/28 [00:01<00:00, 23.43it/s]\n",
      "accuracy: 0.2549, loss: 2.5245 ||: 100%|██████████| 114/114 [00:26<00:00,  3.92it/s]\n",
      "accuracy: 0.1714, loss: 3.3984 ||: 100%|██████████| 28/28 [00:01<00:00, 21.63it/s]\n",
      "accuracy: 0.3573, loss: 2.1293 ||: 100%|██████████| 114/114 [00:27<00:00,  4.02it/s]\n",
      "accuracy: 0.1726, loss: 3.4508 ||: 100%|██████████| 28/28 [00:01<00:00, 15.57it/s]\n",
      "accuracy: 0.4767, loss: 1.7239 ||: 100%|██████████| 114/114 [00:28<00:00,  3.98it/s]\n",
      "accuracy: 0.1989, loss: 3.6930 ||: 100%|██████████| 28/28 [00:01<00:00, 20.52it/s]\n",
      "accuracy: 0.6024, loss: 1.3238 ||: 100%|██████████| 114/114 [00:29<00:00,  3.92it/s]\n",
      "accuracy: 0.2011, loss: 3.7285 ||: 100%|██████████| 28/28 [00:01<00:00, 20.68it/s]\n",
      "accuracy: 0.6973, loss: 1.0205 ||: 100%|██████████| 114/114 [00:26<00:00,  4.23it/s]\n",
      "accuracy: 0.2229, loss: 3.9013 ||: 100%|██████████| 28/28 [00:01<00:00, 24.13it/s]\n",
      "accuracy: 0.7687, loss: 0.7843 ||: 100%|██████████| 114/114 [00:26<00:00,  4.34it/s]\n",
      "accuracy: 0.2183, loss: 4.1265 ||: 100%|██████████| 28/28 [00:01<00:00, 22.02it/s]\n",
      "accuracy: 0.8090, loss: 0.6143 ||: 100%|██████████| 114/114 [00:26<00:00,  4.38it/s]\n",
      "accuracy: 0.2046, loss: 4.3511 ||: 100%|██████████| 28/28 [00:01<00:00, 22.81it/s]\n",
      "accuracy: 0.8345, loss: 0.4895 ||: 100%|██████████| 114/114 [00:25<00:00,  4.38it/s]\n",
      "accuracy: 0.2114, loss: 4.4275 ||: 100%|██████████| 28/28 [00:01<00:00, 23.58it/s]\n",
      "accuracy: 0.8576, loss: 0.4116 ||: 100%|██████████| 114/114 [00:24<00:00,  4.36it/s]\n",
      "accuracy: 0.2126, loss: 4.6365 ||: 100%|██████████| 28/28 [00:01<00:00, 23.56it/s]\n",
      "accuracy: 0.8600, loss: 0.3740 ||: 100%|██████████| 114/114 [00:23<00:00,  4.73it/s]\n",
      "accuracy: 0.2126, loss: 4.6284 ||: 100%|██████████| 28/28 [00:01<00:00, 21.59it/s]\n",
      "accuracy: 0.8688, loss: 0.3192 ||: 100%|██████████| 114/114 [00:23<00:00,  4.48it/s]\n",
      "accuracy: 0.1931, loss: 4.8414 ||: 100%|██████████| 28/28 [00:01<00:00, 20.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 4, 'peak_cpu_memory_MB': 1461.52, 'training_duration': '00:06:31', 'training_start_epoch': 0, 'training_epochs': 13, 'epoch': 13, 'training_accuracy': 0.8600439077936334, 'training_loss': 0.3739910206773825, 'training_cpu_memory_MB': 1461.52, 'validation_accuracy': 0.21257142857142858, 'validation_loss': 4.628362476825714, 'best_validation_accuracy': 0.17142857142857143, 'best_validation_loss': 3.398360252380371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DS = englishData\n",
    "lang = \"english\"\n",
    "prepare(DS,lang)\n",
    "reader = MultilingualDatasetReader()\n",
    "train_dataset = reader.read(str(lang) + 'train.csv')\n",
    "dev_dataset = reader.read(str(lang)+'test.csv')\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "print(optimizer)\n",
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=50)\n",
    "print(trainer.train())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jewelry\n"
     ]
    }
   ],
   "source": [
    "tokens = 'available in itunes about bad wolves a los angeles californiabased heavy metal outfit with an impressive hard rock'\n",
    "predictor = TextClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(tokens)['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estonian Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "578it [00:00, 4258.49it/s]\n",
      "133it [00:00, 4912.18it/s]\n",
      "100%|██████████| 711/711 [00:00<00:00, 9673.20it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=20, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.1125, loss: 2.8812 ||: 100%|██████████| 19/19 [00:02<00:00,  8.90it/s]\n",
      "accuracy: 0.1955, loss: 2.4912 ||: 100%|██████████| 5/5 [00:00<00:00, 43.81it/s]\n",
      "accuracy: 0.1419, loss: 2.4112 ||: 100%|██████████| 19/19 [00:02<00:00,  7.41it/s]\n",
      "accuracy: 0.1429, loss: 2.3065 ||: 100%|██████████| 5/5 [00:00<00:00, 48.07it/s]\n",
      "accuracy: 0.1713, loss: 2.3331 ||: 100%|██████████| 19/19 [00:02<00:00,  6.99it/s]\n",
      "accuracy: 0.1880, loss: 2.3465 ||: 100%|██████████| 5/5 [00:00<00:00, 49.18it/s]\n",
      "accuracy: 0.2630, loss: 2.2167 ||: 100%|██████████| 19/19 [00:02<00:00,  7.88it/s]\n",
      "accuracy: 0.2632, loss: 2.3031 ||: 100%|██████████| 5/5 [00:00<00:00, 48.09it/s]\n",
      "accuracy: 0.2976, loss: 2.0351 ||: 100%|██████████| 19/19 [00:02<00:00,  7.80it/s]\n",
      "accuracy: 0.3910, loss: 2.0688 ||: 100%|██████████| 5/5 [00:00<00:00, 49.04it/s]\n",
      "accuracy: 0.3426, loss: 1.9690 ||: 100%|██████████| 19/19 [00:02<00:00,  7.19it/s]\n",
      "accuracy: 0.3985, loss: 1.9051 ||: 100%|██████████| 5/5 [00:00<00:00, 47.91it/s]\n",
      "accuracy: 0.3806, loss: 1.8130 ||: 100%|██████████| 19/19 [00:02<00:00,  7.95it/s]\n",
      "accuracy: 0.4135, loss: 2.0931 ||: 100%|██████████| 5/5 [00:00<00:00, 47.40it/s]\n",
      "accuracy: 0.4343, loss: 1.7908 ||: 100%|██████████| 19/19 [00:02<00:00,  6.81it/s]\n",
      "accuracy: 0.4361, loss: 1.8727 ||: 100%|██████████| 5/5 [00:00<00:00, 43.09it/s]\n",
      "accuracy: 0.4740, loss: 1.6301 ||: 100%|██████████| 19/19 [00:03<00:00,  5.86it/s]\n",
      "accuracy: 0.4586, loss: 1.8569 ||: 100%|██████████| 5/5 [00:00<00:00, 27.79it/s]\n",
      "accuracy: 0.4481, loss: 1.6104 ||: 100%|██████████| 19/19 [00:02<00:00,  6.95it/s]\n",
      "accuracy: 0.3835, loss: 1.7928 ||: 100%|██████████| 5/5 [00:00<00:00, 41.31it/s]\n",
      "accuracy: 0.5000, loss: 1.4854 ||: 100%|██████████| 19/19 [00:03<00:00,  6.07it/s]\n",
      "accuracy: 0.3985, loss: 1.7850 ||: 100%|██████████| 5/5 [00:00<00:00, 38.54it/s]\n",
      "accuracy: 0.5467, loss: 1.2889 ||: 100%|██████████| 19/19 [00:02<00:00,  6.79it/s]\n",
      "accuracy: 0.4135, loss: 1.7145 ||: 100%|██████████| 5/5 [00:00<00:00, 49.93it/s]\n",
      "accuracy: 0.6021, loss: 1.1810 ||: 100%|██████████| 19/19 [00:02<00:00,  7.27it/s]\n",
      "accuracy: 0.4737, loss: 1.6272 ||: 100%|██████████| 5/5 [00:00<00:00, 39.74it/s]\n",
      "accuracy: 0.6211, loss: 1.0898 ||: 100%|██████████| 19/19 [00:03<00:00,  4.22it/s]\n",
      "accuracy: 0.4211, loss: 1.6405 ||: 100%|██████████| 5/5 [00:00<00:00, 25.02it/s]\n",
      "accuracy: 0.5848, loss: 1.0978 ||: 100%|██████████| 19/19 [00:03<00:00,  5.63it/s]\n",
      "accuracy: 0.4511, loss: 1.6007 ||: 100%|██████████| 5/5 [00:00<00:00, 39.77it/s]\n",
      "accuracy: 0.6574, loss: 0.9512 ||: 100%|██████████| 19/19 [00:02<00:00,  7.35it/s]\n",
      "accuracy: 0.5038, loss: 1.5504 ||: 100%|██████████| 5/5 [00:00<00:00, 48.04it/s]\n",
      "accuracy: 0.6713, loss: 0.9079 ||: 100%|██████████| 19/19 [00:02<00:00,  6.66it/s]\n",
      "accuracy: 0.4436, loss: 1.6806 ||: 100%|██████████| 5/5 [00:00<00:00, 46.05it/s]\n",
      "accuracy: 0.7024, loss: 0.8767 ||: 100%|██████████| 19/19 [00:02<00:00,  7.59it/s]\n",
      "accuracy: 0.4511, loss: 1.6952 ||: 100%|██████████| 5/5 [00:00<00:00, 33.91it/s]\n",
      "accuracy: 0.7111, loss: 0.7720 ||: 100%|██████████| 19/19 [00:02<00:00,  7.83it/s]\n",
      "accuracy: 0.4511, loss: 1.8568 ||: 100%|██████████| 5/5 [00:00<00:00, 47.05it/s]\n",
      "accuracy: 0.7370, loss: 0.6889 ||: 100%|██████████| 19/19 [00:03<00:00,  4.63it/s]\n",
      "accuracy: 0.4286, loss: 1.7159 ||: 100%|██████████| 5/5 [00:00<00:00, 40.51it/s]\n",
      "accuracy: 0.7664, loss: 0.6943 ||: 100%|██████████| 19/19 [00:02<00:00,  5.09it/s]\n",
      "accuracy: 0.4361, loss: 1.6901 ||: 100%|██████████| 5/5 [00:00<00:00, 31.59it/s]\n",
      "accuracy: 0.7405, loss: 0.6463 ||: 100%|██████████| 19/19 [00:02<00:00,  6.68it/s]\n",
      "accuracy: 0.4060, loss: 1.8440 ||: 100%|██████████| 5/5 [00:00<00:00, 48.38it/s]\n",
      "accuracy: 0.7578, loss: 0.6153 ||: 100%|██████████| 19/19 [00:03<00:00,  4.24it/s]\n",
      "accuracy: 0.3684, loss: 1.7836 ||: 100%|██████████| 5/5 [00:00<00:00, 36.22it/s]\n",
      "accuracy: 0.7630, loss: 0.5924 ||: 100%|██████████| 19/19 [00:03<00:00,  6.20it/s]\n",
      "accuracy: 0.2932, loss: 1.9600 ||: 100%|██████████| 5/5 [00:00<00:00, 31.71it/s]\n",
      "accuracy: 0.7716, loss: 0.5849 ||: 100%|██████████| 19/19 [00:02<00:00,  7.17it/s]\n",
      "accuracy: 0.4361, loss: 1.9164 ||: 100%|██████████| 5/5 [00:00<00:00, 48.50it/s]\n",
      "accuracy: 0.7941, loss: 0.5328 ||: 100%|██████████| 19/19 [00:02<00:00,  6.78it/s]\n",
      "accuracy: 0.3985, loss: 2.1364 ||: 100%|██████████| 5/5 [00:00<00:00, 47.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 15, 'peak_cpu_memory_MB': 809.384, 'training_duration': '00:01:13', 'training_start_epoch': 0, 'training_epochs': 24, 'epoch': 24, 'training_accuracy': 0.7716262975778547, 'training_loss': 0.5849381755841406, 'training_cpu_memory_MB': 809.232, 'validation_accuracy': 0.43609022556390975, 'validation_loss': 1.9164239645004273, 'best_validation_accuracy': 0.5037593984962406, 'best_validation_loss': 1.5504174709320069}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainLang(estonianData,\"estonian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1609it [00:00, 2254.28it/s]\n",
      "383it [00:00, 6246.43it/s]\n",
      "100%|██████████| 1992/1992 [00:00<00:00, 14365.29it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=28, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0740, loss: 3.2149 ||: 100%|██████████| 51/51 [00:08<00:00,  5.94it/s]\n",
      "accuracy: 0.0888, loss: 3.1456 ||: 100%|██████████| 12/12 [00:00<00:00, 31.00it/s]\n",
      "accuracy: 0.0901, loss: 3.0734 ||: 100%|██████████| 51/51 [00:07<00:00,  6.84it/s]\n",
      "accuracy: 0.0914, loss: 3.0363 ||: 100%|██████████| 12/12 [00:00<00:00, 33.05it/s]\n",
      "accuracy: 0.1175, loss: 2.8930 ||: 100%|██████████| 51/51 [00:07<00:00,  6.99it/s]\n",
      "accuracy: 0.1175, loss: 2.8830 ||: 100%|██████████| 12/12 [00:00<00:00, 38.58it/s]\n",
      "accuracy: 0.2039, loss: 2.5863 ||: 100%|██████████| 51/51 [00:07<00:00,  8.67it/s]\n",
      "accuracy: 0.1540, loss: 2.8339 ||: 100%|██████████| 12/12 [00:00<00:00, 39.55it/s]\n",
      "accuracy: 0.3132, loss: 2.2385 ||: 100%|██████████| 51/51 [00:07<00:00,  6.40it/s]\n",
      "accuracy: 0.2089, loss: 2.7449 ||: 100%|██████████| 12/12 [00:00<00:00, 37.74it/s]\n",
      "accuracy: 0.4786, loss: 1.8214 ||: 100%|██████████| 51/51 [00:08<00:00,  6.80it/s]\n",
      "accuracy: 0.2402, loss: 2.7302 ||: 100%|██████████| 12/12 [00:00<00:00, 34.92it/s]\n",
      "accuracy: 0.6296, loss: 1.3799 ||: 100%|██████████| 51/51 [00:08<00:00,  7.28it/s]\n",
      "accuracy: 0.2637, loss: 2.8214 ||: 100%|██████████| 12/12 [00:00<00:00, 39.91it/s]\n",
      "accuracy: 0.7427, loss: 0.9498 ||: 100%|██████████| 51/51 [00:08<00:00,  6.24it/s]\n",
      "accuracy: 0.2611, loss: 2.9643 ||: 100%|██████████| 12/12 [00:00<00:00, 36.71it/s]\n",
      "accuracy: 0.8154, loss: 0.6890 ||: 100%|██████████| 51/51 [00:07<00:00,  6.82it/s]\n",
      "accuracy: 0.2742, loss: 3.0261 ||: 100%|██████████| 12/12 [00:00<00:00, 40.26it/s]\n",
      "accuracy: 0.8533, loss: 0.5156 ||: 100%|██████████| 51/51 [00:08<00:00,  6.77it/s]\n",
      "accuracy: 0.2689, loss: 3.1232 ||: 100%|██████████| 12/12 [00:00<00:00, 39.23it/s]\n",
      "accuracy: 0.8800, loss: 0.3784 ||: 100%|██████████| 51/51 [00:07<00:00,  6.93it/s]\n",
      "accuracy: 0.2898, loss: 3.2544 ||: 100%|██████████| 12/12 [00:00<00:00, 40.25it/s]\n",
      "accuracy: 0.8875, loss: 0.3180 ||: 100%|██████████| 51/51 [00:07<00:00,  6.12it/s]\n",
      "accuracy: 0.2715, loss: 3.2919 ||: 100%|██████████| 12/12 [00:00<00:00, 38.47it/s]\n",
      "accuracy: 0.8912, loss: 0.2845 ||: 100%|██████████| 51/51 [00:07<00:00,  6.63it/s]\n",
      "accuracy: 0.2768, loss: 3.2769 ||: 100%|██████████| 12/12 [00:00<00:00, 39.25it/s]\n",
      "accuracy: 0.8981, loss: 0.2455 ||: 100%|██████████| 51/51 [00:06<00:00,  6.34it/s]\n",
      "accuracy: 0.2924, loss: 3.2798 ||: 100%|██████████| 12/12 [00:00<00:00, 39.11it/s]\n",
      "accuracy: 0.9055, loss: 0.2267 ||: 100%|██████████| 51/51 [00:07<00:00,  6.15it/s]\n",
      "accuracy: 0.2846, loss: 3.4460 ||: 100%|██████████| 12/12 [00:00<00:00, 39.72it/s]\n",
      "accuracy: 0.9124, loss: 0.2068 ||: 100%|██████████| 51/51 [00:09<00:00,  5.49it/s]\n",
      "accuracy: 0.2663, loss: 3.4767 ||: 100%|██████████| 12/12 [00:00<00:00, 30.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 5, 'peak_cpu_memory_MB': 848.4, 'training_duration': '00:02:02', 'training_start_epoch': 0, 'training_epochs': 14, 'epoch': 14, 'training_accuracy': 0.9055313859540087, 'training_loss': 0.22668769312839882, 'training_cpu_memory_MB': 848.4, 'validation_accuracy': 0.2845953002610966, 'validation_loss': 3.446045736471812, 'best_validation_accuracy': 0.2402088772845953, 'best_validation_loss': 2.730215867360433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang = \"arabic\"\n",
    "DS = arabicData\n",
    "trainLang(DS,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3235it [00:00, 3610.27it/s]\n",
      "805it [00:00, 3072.66it/s]\n",
      "100%|██████████| 4040/4040 [00:00<00:00, 13888.46it/s]\n",
      "  0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=42, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0457, loss: 3.6906 ||: 100%|██████████| 102/102 [00:16<00:00,  6.07it/s]\n",
      "accuracy: 0.0522, loss: 3.6524 ||: 100%|██████████| 26/26 [00:00<00:00, 35.35it/s]\n",
      "accuracy: 0.0668, loss: 3.5939 ||: 100%|██████████| 102/102 [00:15<00:00,  7.23it/s]\n",
      "accuracy: 0.0733, loss: 3.5352 ||: 100%|██████████| 26/26 [00:00<00:00, 41.17it/s]\n",
      "accuracy: 0.1286, loss: 3.2442 ||: 100%|██████████| 102/102 [00:14<00:00,  6.99it/s]\n",
      "accuracy: 0.1342, loss: 3.3428 ||: 100%|██████████| 26/26 [00:00<00:00, 36.15it/s]\n",
      "accuracy: 0.2340, loss: 2.8023 ||: 100%|██████████| 102/102 [00:15<00:00,  6.16it/s]\n",
      "accuracy: 0.1764, loss: 3.2250 ||: 100%|██████████| 26/26 [00:00<00:00, 39.56it/s]\n",
      "accuracy: 0.3431, loss: 2.3890 ||: 100%|██████████| 102/102 [00:14<00:00,  6.37it/s]\n",
      "accuracy: 0.2012, loss: 3.3317 ||: 100%|██████████| 26/26 [00:00<00:00, 40.85it/s]\n",
      "accuracy: 0.4906, loss: 1.8463 ||: 100%|██████████| 102/102 [00:14<00:00,  7.07it/s]\n",
      "accuracy: 0.2099, loss: 3.5232 ||: 100%|██████████| 26/26 [00:00<00:00, 41.37it/s]\n",
      "accuracy: 0.6405, loss: 1.3406 ||: 100%|██████████| 102/102 [00:14<00:00,  7.16it/s]\n",
      "accuracy: 0.2137, loss: 3.8539 ||: 100%|██████████| 26/26 [00:00<00:00, 42.12it/s]\n",
      "accuracy: 0.7369, loss: 0.9643 ||: 100%|██████████| 102/102 [00:14<00:00,  6.65it/s]\n",
      "accuracy: 0.2348, loss: 4.0151 ||: 100%|██████████| 26/26 [00:00<00:00, 41.50it/s]\n",
      "accuracy: 0.8173, loss: 0.6813 ||: 100%|██████████| 102/102 [00:14<00:00,  6.66it/s]\n",
      "accuracy: 0.2261, loss: 4.3507 ||: 100%|██████████| 26/26 [00:00<00:00, 39.70it/s]\n",
      "accuracy: 0.8640, loss: 0.5067 ||: 100%|██████████| 102/102 [00:14<00:00,  5.39it/s]\n",
      "accuracy: 0.2323, loss: 4.4697 ||: 100%|██████████| 26/26 [00:00<00:00, 40.38it/s]\n",
      "accuracy: 0.8921, loss: 0.3746 ||: 100%|██████████| 102/102 [00:14<00:00,  7.34it/s]\n",
      "accuracy: 0.2323, loss: 4.5871 ||: 100%|██████████| 26/26 [00:00<00:00, 41.26it/s]\n",
      "accuracy: 0.9017, loss: 0.3195 ||: 100%|██████████| 102/102 [00:14<00:00,  6.67it/s]\n",
      "accuracy: 0.2186, loss: 4.8656 ||: 100%|██████████| 26/26 [00:00<00:00, 41.47it/s]\n",
      "accuracy: 0.9221, loss: 0.2385 ||: 100%|██████████| 102/102 [00:16<00:00,  6.43it/s]\n",
      "accuracy: 0.2236, loss: 5.1006 ||: 100%|██████████| 26/26 [00:00<00:00, 41.42it/s]\n",
      "accuracy: 0.9230, loss: 0.2138 ||: 100%|██████████| 102/102 [00:15<00:00,  6.24it/s]\n",
      "accuracy: 0.2186, loss: 5.1629 ||: 100%|██████████| 26/26 [00:00<00:00, 39.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 3, 'peak_cpu_memory_MB': 857.904, 'training_duration': '00:03:25', 'training_start_epoch': 0, 'training_epochs': 12, 'epoch': 12, 'training_accuracy': 0.9221020092735703, 'training_loss': 0.23847985302335492, 'training_cpu_memory_MB': 857.904, 'validation_accuracy': 0.2236024844720497, 'validation_loss': 5.1005976016704855, 'best_validation_accuracy': 0.1763975155279503, 'best_validation_loss': 3.22501749258775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang = \"spanish\"\n",
    "DS = spanishData\n",
    "trainLang(DS,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3107it [00:00, 3646.04it/s]\n",
      "766it [00:00, 3557.22it/s]\n",
      "100%|██████████| 3873/3873 [00:00<00:00, 20387.45it/s]\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=41, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0447, loss: 3.6517 ||: 100%|██████████| 98/98 [00:14<00:00,  6.41it/s]\n",
      "accuracy: 0.0496, loss: 3.6294 ||: 100%|██████████| 24/24 [00:00<00:00, 37.29it/s]\n",
      "accuracy: 0.0846, loss: 3.5051 ||: 100%|██████████| 98/98 [00:14<00:00,  7.25it/s]\n",
      "accuracy: 0.0940, loss: 3.4574 ||: 100%|██████████| 24/24 [00:00<00:00, 37.05it/s]\n",
      "accuracy: 0.1561, loss: 3.1613 ||: 100%|██████████| 98/98 [00:14<00:00,  6.31it/s]\n",
      "accuracy: 0.1527, loss: 3.2518 ||: 100%|██████████| 24/24 [00:00<00:00, 38.60it/s]\n",
      "accuracy: 0.2617, loss: 2.7120 ||: 100%|██████████| 98/98 [00:14<00:00,  6.76it/s]\n",
      "accuracy: 0.1710, loss: 3.1620 ||: 100%|██████████| 24/24 [00:00<00:00, 37.26it/s]\n",
      "accuracy: 0.3746, loss: 2.2211 ||: 100%|██████████| 98/98 [00:14<00:00,  6.57it/s]\n",
      "accuracy: 0.1958, loss: 3.1418 ||: 100%|██████████| 24/24 [00:00<00:00, 41.30it/s]\n",
      "accuracy: 0.4892, loss: 1.7936 ||: 100%|██████████| 98/98 [00:14<00:00,  6.07it/s]\n",
      "accuracy: 0.2167, loss: 3.2966 ||: 100%|██████████| 24/24 [00:00<00:00, 39.20it/s]\n",
      "accuracy: 0.6154, loss: 1.3429 ||: 100%|██████████| 98/98 [00:14<00:00,  6.16it/s]\n",
      "accuracy: 0.2298, loss: 3.4552 ||: 100%|██████████| 24/24 [00:00<00:00, 36.00it/s]\n",
      "accuracy: 0.7016, loss: 1.0394 ||: 100%|██████████| 98/98 [00:14<00:00,  8.00it/s]\n",
      "accuracy: 0.2193, loss: 3.7655 ||: 100%|██████████| 24/24 [00:00<00:00, 41.66it/s]\n",
      "accuracy: 0.7831, loss: 0.7560 ||: 100%|██████████| 98/98 [00:14<00:00,  6.59it/s]\n",
      "accuracy: 0.2141, loss: 4.0896 ||: 100%|██████████| 24/24 [00:00<00:00, 36.29it/s]\n",
      "accuracy: 0.8227, loss: 0.6194 ||: 100%|██████████| 98/98 [00:14<00:00,  6.81it/s]\n",
      "accuracy: 0.2285, loss: 4.1194 ||: 100%|██████████| 24/24 [00:00<00:00, 40.42it/s]\n",
      "accuracy: 0.8458, loss: 0.4728 ||: 100%|██████████| 98/98 [00:13<00:00,  8.11it/s]\n",
      "accuracy: 0.2141, loss: 4.3199 ||: 100%|██████████| 24/24 [00:00<00:00, 41.07it/s]\n",
      "accuracy: 0.8648, loss: 0.3745 ||: 100%|██████████| 98/98 [00:13<00:00,  7.43it/s]\n",
      "accuracy: 0.2272, loss: 4.5051 ||: 100%|██████████| 24/24 [00:00<00:00, 40.94it/s]\n",
      "accuracy: 0.8761, loss: 0.3165 ||: 100%|██████████| 98/98 [00:14<00:00,  5.99it/s]\n",
      "accuracy: 0.2167, loss: 4.6221 ||: 100%|██████████| 24/24 [00:00<00:00, 40.37it/s]\n",
      "accuracy: 0.8790, loss: 0.2933 ||: 100%|██████████| 98/98 [00:15<00:00,  7.03it/s]\n",
      "accuracy: 0.2076, loss: 4.6400 ||: 100%|██████████| 24/24 [00:00<00:00, 41.28it/s]\n",
      "accuracy: 0.8867, loss: 0.2726 ||: 100%|██████████| 98/98 [00:14<00:00,  6.90it/s]\n",
      "accuracy: 0.2193, loss: 4.8373 ||: 100%|██████████| 24/24 [00:00<00:00, 40.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 4, 'peak_cpu_memory_MB': 880.408, 'training_duration': '00:03:30', 'training_start_epoch': 0, 'training_epochs': 13, 'epoch': 13, 'training_accuracy': 0.878982941744448, 'training_loss': 0.29328808142822616, 'training_cpu_memory_MB': 880.408, 'validation_accuracy': 0.20757180156657964, 'validation_loss': 4.639977211753528, 'best_validation_accuracy': 0.195822454308094, 'best_validation_loss': 3.1418435921271644}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang = \"french\"\n",
    "DS = frenchData\n",
    "trainLang(DS,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase II: Classification for all languages using one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Concatenation for all the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllLangs = englishData.append(estonianData)\n",
    "AllLangs = AllLangs.append(arabicData)\n",
    "AllLangs = AllLangs.append(frenchData)\n",
    "AllLangs = AllLangs.append(spanishData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All langugues are concatenated and the dataset splited into 80, 20 for training and 20% from all languages was selected as validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12059it [00:02, 4253.97it/s]\n",
      "3076it [00:00, 5113.82it/s] \n",
      "100%|██████████| 15135/15135 [00:00<00:00, 18087.20it/s]\n",
      "  0%|          | 0/377 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0595, loss: 3.6332 ||: 100%|██████████| 377/377 [01:09<00:00,  5.68it/s]\n",
      "accuracy: 0.0809, loss: 3.4967 ||: 100%|██████████| 97/97 [00:02<00:00, 37.69it/s]\n",
      "accuracy: 0.1140, loss: 3.2527 ||: 100%|██████████| 377/377 [01:13<00:00,  4.53it/s]\n",
      "accuracy: 0.1359, loss: 3.2834 ||: 100%|██████████| 97/97 [00:02<00:00, 37.70it/s]\n",
      "accuracy: 0.2103, loss: 2.7668 ||: 100%|██████████| 377/377 [01:11<00:00,  5.22it/s]\n",
      "accuracy: 0.1869, loss: 3.2044 ||: 100%|██████████| 97/97 [00:02<00:00, 42.40it/s]\n",
      "accuracy: 0.3435, loss: 2.2126 ||: 100%|██████████| 377/377 [01:22<00:00,  4.94it/s]\n",
      "accuracy: 0.2178, loss: 3.2420 ||: 100%|██████████| 97/97 [00:02<00:00, 37.92it/s]\n",
      "accuracy: 0.5004, loss: 1.6371 ||: 100%|██████████| 377/377 [01:10<00:00,  4.95it/s]\n",
      "accuracy: 0.2263, loss: 3.4114 ||: 100%|██████████| 97/97 [00:02<00:00, 38.70it/s]\n",
      "accuracy: 0.6354, loss: 1.1587 ||: 100%|██████████| 377/377 [01:10<00:00,  5.08it/s]\n",
      "accuracy: 0.2341, loss: 3.7145 ||: 100%|██████████| 97/97 [00:02<00:00, 39.11it/s]\n",
      "accuracy: 0.7490, loss: 0.7998 ||: 100%|██████████| 377/377 [01:10<00:00,  5.17it/s]\n",
      "accuracy: 0.2334, loss: 3.9719 ||: 100%|██████████| 97/97 [00:02<00:00, 38.61it/s]\n",
      "accuracy: 0.8120, loss: 0.5793 ||: 100%|██████████| 377/377 [01:12<00:00,  3.92it/s]\n",
      "accuracy: 0.2279, loss: 4.1126 ||: 100%|██████████| 97/97 [00:02<00:00, 37.44it/s]\n",
      "accuracy: 0.8478, loss: 0.4443 ||: 100%|██████████| 377/377 [01:15<00:00,  4.96it/s]\n",
      "accuracy: 0.2276, loss: 4.4584 ||: 100%|██████████| 97/97 [00:02<00:00, 33.99it/s]\n",
      "accuracy: 0.8661, loss: 0.3653 ||: 100%|██████████| 377/377 [01:14<00:00,  4.43it/s]\n",
      "accuracy: 0.2367, loss: 4.4980 ||: 100%|██████████| 97/97 [00:02<00:00, 32.38it/s]\n",
      "accuracy: 0.8790, loss: 0.3141 ||: 100%|██████████| 377/377 [01:11<00:00,  5.05it/s]\n",
      "accuracy: 0.2315, loss: 4.6591 ||: 100%|██████████| 97/97 [00:02<00:00, 38.75it/s]\n",
      "accuracy: 0.8826, loss: 0.2860 ||: 100%|██████████| 377/377 [01:10<00:00,  5.19it/s]\n",
      "accuracy: 0.2311, loss: 4.7431 ||: 100%|██████████| 97/97 [00:02<00:00, 38.62it/s]\n",
      "accuracy: 0.8842, loss: 0.2771 ||: 100%|██████████| 377/377 [01:14<00:00,  5.18it/s]\n",
      "accuracy: 0.2266, loss: 4.6541 ||: 100%|██████████| 97/97 [00:02<00:00, 38.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 2, 'peak_cpu_memory_MB': 1233.208, 'training_duration': '00:15:04', 'training_start_epoch': 0, 'training_epochs': 11, 'epoch': 11, 'training_accuracy': 0.8825773281366615, 'training_loss': 0.285989127242913, 'training_cpu_memory_MB': 1233.208, 'validation_accuracy': 0.23114434330299088, 'validation_loss': 4.743071145618085, 'best_validation_accuracy': 0.18693107932379713, 'best_validation_loss': 3.204392076767597}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang = \"allLanguages\"\n",
    "DS = AllLangs\n",
    "trainLang(DS,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation using a subset from the english dataset, which was excluded before spliting the whole dataframe into train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAllLang(DS,lang,testPath):\n",
    "    DS['text'] = preprocessing(DS['text'])\n",
    "    DS = DS[[\"label\",\"text\"]]\n",
    "    df = pd.DataFrame(DS[\"label\"]) \n",
    "    df[\"text\"] = DS['text'].str.split().str[1:100].str.join(\" \")\n",
    "    \n",
    "    \n",
    "    df.to_csv(str(lang)+\"train.csv\",index=False,header=False)\n",
    "\n",
    "    reader = MultilingualDatasetReader()\n",
    "    train_dataset = reader.read(str(lang) + 'train.csv')\n",
    "    dev_dataset = reader.read(testPath)\n",
    "\n",
    "    vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                      min_count={'tokens': 3})\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=EMBEDDING_DIM)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "    encoder = PytorchSeq2VecWrapper(\n",
    "        torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "    model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "    print(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    print(optimizer)\n",
    "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=dev_dataset,\n",
    "                      patience=10,\n",
    "                      num_epochs=50)\n",
    "    print(trainer.train())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14215it [00:03, 4047.59it/s]\n",
      "925it [00:00, 10890.96it/s]\n",
      "100%|██████████| 15140/15140 [00:00<00:00, 19460.59it/s]\n",
      "  0%|          | 0/445 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0607, loss: 3.6050 ||: 100%|██████████| 445/445 [01:22<00:00,  5.27it/s]\n",
      "accuracy: 0.0314, loss: 3.8596 ||: 100%|██████████| 29/29 [00:00<00:00, 38.11it/s]\n",
      "accuracy: 0.1213, loss: 3.2738 ||: 100%|██████████| 445/445 [01:21<00:00,  5.35it/s]\n",
      "accuracy: 0.1514, loss: 3.1513 ||: 100%|██████████| 29/29 [00:00<00:00, 42.64it/s]\n",
      "accuracy: 0.2103, loss: 2.8005 ||: 100%|██████████| 445/445 [01:25<00:00,  6.05it/s]\n",
      "accuracy: 0.2400, loss: 2.7640 ||: 100%|██████████| 29/29 [00:00<00:00, 42.72it/s]\n",
      "accuracy: 0.3037, loss: 2.3737 ||: 100%|██████████| 445/445 [01:37<00:00,  5.06it/s]\n",
      "accuracy: 0.3297, loss: 2.4199 ||: 100%|██████████| 29/29 [00:00<00:00, 30.74it/s]\n",
      "accuracy: 0.4096, loss: 1.9470 ||: 100%|██████████| 445/445 [01:30<00:00,  5.56it/s]\n",
      "accuracy: 0.4573, loss: 2.1025 ||: 100%|██████████| 29/29 [00:00<00:00, 40.76it/s]\n",
      "accuracy: 0.5197, loss: 1.5586 ||: 100%|██████████| 445/445 [01:32<00:00,  4.49it/s]\n",
      "accuracy: 0.5341, loss: 1.8408 ||: 100%|██████████| 29/29 [00:00<00:00, 36.59it/s]\n",
      "accuracy: 0.6384, loss: 1.1792 ||: 100%|██████████| 445/445 [01:33<00:00,  4.74it/s]\n",
      "accuracy: 0.5914, loss: 1.6653 ||: 100%|██████████| 29/29 [00:00<00:00, 34.74it/s]\n",
      "accuracy: 0.7293, loss: 0.8725 ||: 100%|██████████| 445/445 [01:25<00:00,  5.41it/s]\n",
      "accuracy: 0.6605, loss: 1.4624 ||: 100%|██████████| 29/29 [00:00<00:00, 41.64it/s]\n",
      "accuracy: 0.7926, loss: 0.6528 ||: 100%|██████████| 445/445 [01:28<00:00,  5.40it/s]\n",
      "accuracy: 0.6908, loss: 1.3867 ||: 100%|██████████| 29/29 [00:00<00:00, 33.17it/s]\n",
      "accuracy: 0.8193, loss: 0.5291 ||: 100%|██████████| 445/445 [01:30<00:00,  5.00it/s]\n",
      "accuracy: 0.7308, loss: 1.2486 ||: 100%|██████████| 29/29 [00:00<00:00, 40.88it/s]\n",
      "accuracy: 0.8429, loss: 0.4425 ||: 100%|██████████| 445/445 [01:22<00:00,  5.22it/s]\n",
      "accuracy: 0.7481, loss: 1.2229 ||: 100%|██████████| 29/29 [00:00<00:00, 40.83it/s]\n",
      "accuracy: 0.8557, loss: 0.3807 ||: 100%|██████████| 445/445 [01:22<00:00,  5.01it/s]\n",
      "accuracy: 0.7470, loss: 1.2319 ||: 100%|██████████| 29/29 [00:00<00:00, 41.42it/s]\n",
      "accuracy: 0.8671, loss: 0.3467 ||: 100%|██████████| 445/445 [01:21<00:00,  5.29it/s]\n",
      "accuracy: 0.7546, loss: 1.2162 ||: 100%|██████████| 29/29 [00:00<00:00, 41.06it/s]\n",
      "accuracy: 0.8693, loss: 0.3372 ||: 100%|██████████| 445/445 [01:24<00:00,  5.24it/s]\n",
      "accuracy: 0.7492, loss: 1.2081 ||: 100%|██████████| 29/29 [00:00<00:00, 35.04it/s]\n",
      "accuracy: 0.8700, loss: 0.3111 ||: 100%|██████████| 445/445 [01:24<00:00,  5.33it/s]\n",
      "accuracy: 0.7535, loss: 1.2326 ||: 100%|██████████| 29/29 [00:00<00:00, 35.22it/s]\n",
      "accuracy: 0.8732, loss: 0.3003 ||: 100%|██████████| 445/445 [01:24<00:00,  5.22it/s]\n",
      "accuracy: 0.7492, loss: 1.1917 ||: 100%|██████████| 29/29 [00:00<00:00, 36.80it/s]\n",
      "accuracy: 0.8759, loss: 0.2803 ||: 100%|██████████| 445/445 [01:25<00:00,  5.12it/s]\n",
      "accuracy: 0.7546, loss: 1.2431 ||: 100%|██████████| 29/29 [00:00<00:00, 35.37it/s]\n",
      "accuracy: 0.8698, loss: 0.2934 ||: 100%|██████████| 445/445 [01:25<00:00,  5.09it/s]\n",
      "accuracy: 0.7492, loss: 1.2013 ||: 100%|██████████| 29/29 [00:00<00:00, 35.62it/s]\n",
      "accuracy: 0.8760, loss: 0.2631 ||: 100%|██████████| 445/445 [01:25<00:00,  4.88it/s]\n",
      "accuracy: 0.7524, loss: 1.2315 ||: 100%|██████████| 29/29 [00:00<00:00, 34.99it/s]\n",
      "accuracy: 0.8810, loss: 0.2515 ||: 100%|██████████| 445/445 [01:28<00:00,  4.19it/s]\n",
      "accuracy: 0.7492, loss: 1.2820 ||: 100%|██████████| 29/29 [00:01<00:00, 25.46it/s]\n",
      "accuracy: 0.8796, loss: 0.2506 ||: 100%|██████████| 445/445 [01:44<00:00,  4.06it/s]\n",
      "accuracy: 0.7632, loss: 1.2649 ||: 100%|██████████| 29/29 [00:01<00:00, 23.27it/s]\n",
      "accuracy: 0.8770, loss: 0.2482 ||: 100%|██████████| 445/445 [01:42<00:00,  4.22it/s]\n",
      "accuracy: 0.7589, loss: 1.1898 ||: 100%|██████████| 29/29 [00:01<00:00, 23.19it/s]\n",
      "accuracy: 0.8787, loss: 0.2471 ||: 100%|██████████| 445/445 [01:42<00:00,  4.00it/s]\n",
      "accuracy: 0.7351, loss: 1.2328 ||: 100%|██████████| 29/29 [00:01<00:00, 23.47it/s]\n",
      "accuracy: 0.8779, loss: 0.2484 ||: 100%|██████████| 445/445 [01:41<00:00,  5.30it/s]\n",
      "accuracy: 0.7265, loss: 1.3542 ||: 100%|██████████| 29/29 [00:01<00:00, 22.55it/s]\n",
      "accuracy: 0.8741, loss: 0.2528 ||: 100%|██████████| 445/445 [01:50<00:00,  3.89it/s]\n",
      "accuracy: 0.7481, loss: 1.2288 ||: 100%|██████████| 29/29 [00:01<00:00, 22.60it/s]\n",
      "accuracy: 0.8814, loss: 0.2256 ||: 100%|██████████| 445/445 [01:55<00:00,  3.86it/s]\n",
      "accuracy: 0.7568, loss: 1.1703 ||: 100%|██████████| 29/29 [00:01<00:00, 18.34it/s]\n",
      "accuracy: 0.8849, loss: 0.2199 ||: 100%|██████████| 445/445 [01:59<00:00,  3.69it/s]\n",
      "accuracy: 0.7578, loss: 1.1933 ||: 100%|██████████| 29/29 [00:01<00:00, 20.39it/s]\n",
      "accuracy: 0.8825, loss: 0.2183 ||: 100%|██████████| 445/445 [01:55<00:00,  4.15it/s]\n",
      "accuracy: 0.7481, loss: 1.2003 ||: 100%|██████████| 29/29 [00:01<00:00, 21.98it/s]\n",
      "accuracy: 0.8832, loss: 0.2238 ||: 100%|██████████| 445/445 [02:15<00:00,  3.37it/s]\n",
      "accuracy: 0.7643, loss: 1.1967 ||: 100%|██████████| 29/29 [00:02<00:00, 14.44it/s]\n",
      "accuracy: 0.8863, loss: 0.2195 ||: 100%|██████████| 445/445 [02:00<00:00,  4.27it/s]\n",
      "accuracy: 0.7611, loss: 1.1702 ||: 100%|██████████| 29/29 [00:01<00:00, 22.39it/s]\n",
      "accuracy: 0.8806, loss: 0.2214 ||: 100%|██████████| 445/445 [01:46<00:00,  4.21it/s]\n",
      "accuracy: 0.7643, loss: 1.2175 ||: 100%|██████████| 29/29 [00:01<00:00, 23.27it/s]\n",
      "accuracy: 0.8829, loss: 0.2206 ||: 100%|██████████| 445/445 [01:41<00:00,  4.43it/s]\n",
      "accuracy: 0.7643, loss: 1.2031 ||: 100%|██████████| 29/29 [00:01<00:00, 23.36it/s]\n",
      "accuracy: 0.8791, loss: 0.2340 ||: 100%|██████████| 445/445 [01:46<00:00,  4.38it/s]\n",
      "accuracy: 0.7632, loss: 1.2021 ||: 100%|██████████| 29/29 [00:01<00:00, 22.54it/s]\n",
      "accuracy: 0.8836, loss: 0.2234 ||: 100%|██████████| 445/445 [01:49<00:00,  4.50it/s]\n",
      "accuracy: 0.7676, loss: 1.2137 ||: 100%|██████████| 29/29 [00:01<00:00, 23.12it/s]\n",
      "accuracy: 0.8862, loss: 0.2009 ||: 100%|██████████| 445/445 [01:48<00:00,  4.42it/s]\n",
      "accuracy: 0.7665, loss: 1.2001 ||: 100%|██████████| 29/29 [00:01<00:00, 24.82it/s]\n",
      "accuracy: 0.8868, loss: 0.1922 ||: 100%|██████████| 445/445 [01:38<00:00,  4.52it/s]\n",
      "accuracy: 0.7546, loss: 1.2448 ||: 100%|██████████| 29/29 [00:01<00:00, 25.26it/s]\n",
      "accuracy: 0.8889, loss: 0.1933 ||: 100%|██████████| 445/445 [01:38<00:00,  4.36it/s]\n",
      "accuracy: 0.7665, loss: 1.2178 ||: 100%|██████████| 29/29 [00:01<00:00, 25.50it/s]\n",
      "accuracy: 0.8846, loss: 0.2102 ||: 100%|██████████| 445/445 [01:38<00:00,  4.75it/s]\n",
      "accuracy: 0.7676, loss: 1.2404 ||: 100%|██████████| 29/29 [00:01<00:00, 25.34it/s]\n",
      "accuracy: 0.8846, loss: 0.2236 ||: 100%|██████████| 445/445 [01:38<00:00,  4.43it/s]\n",
      "accuracy: 0.7654, loss: 1.1931 ||: 100%|██████████| 29/29 [00:01<00:00, 25.08it/s]\n",
      "accuracy: 0.8839, loss: 0.2136 ||: 100%|██████████| 445/445 [01:37<00:00,  4.19it/s]\n",
      "accuracy: 0.7578, loss: 1.2108 ||: 100%|██████████| 29/29 [00:01<00:00, 25.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 29, 'peak_cpu_memory_MB': 1461.52, 'training_duration': '01:03:51', 'training_start_epoch': 0, 'training_epochs': 38, 'epoch': 38, 'training_accuracy': 0.8846289131199437, 'training_loss': 0.22364689901973425, 'training_cpu_memory_MB': 1461.52, 'validation_accuracy': 0.7654054054054054, 'validation_loss': 1.1931363056445945, 'best_validation_accuracy': 0.7610810810810811, 'best_validation_loss': 1.1701805684073219}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "englishTrain = pd.read_csv(\"englishtrainForAll.csv\") \n",
    "AllLangs = englishTrain.append(estonianData)\n",
    "AllLangs = AllLangs.append(arabicData)\n",
    "AllLangs = AllLangs.append(frenchData)\n",
    "AllLangs = AllLangs.append(spanishData)\n",
    "\n",
    "testPath = 'englishtest.csv'\n",
    "trainAllLang(AllLangs,\"AllLangs2\",testPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase III: Round Trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset that have the arabic text translated to english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>NewEnglish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advertising</td>\n",
       "      <td>asks sec to mull halfyear corporate filings vs...</td>\n",
       "      <td>يسأل ثانية للنظر في طلبات الشركات نصف السنة مق...</td>\n",
       "      <td>Asked again to look at half-year corporate req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advertising</td>\n",
       "      <td>st up on trade hopes sp equals longest bull ru...</td>\n",
       "      <td>التباطؤ في الآمال التجارية: sp تساوي أطول فترة...</td>\n",
       "      <td>Deceleration in business hopes: sp equals the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advertising</td>\n",
       "      <td>shares hit oneyear low on turkey china worries...</td>\n",
       "      <td>الأسهم الصينية تتراجع إلى أدنى مستوياتها مع ال...</td>\n",
       "      <td>Chinese stocks fall to their lowest levels wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advertising</td>\n",
       "      <td>stocks weaken as turkey worries weigh dollar s...</td>\n",
       "      <td>الأسهم الأسيوية تضعف مع قلق تركيا من ارتفاع ال...</td>\n",
       "      <td>Asian stocks weaken as Turkey worries about do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>advertising</td>\n",
       "      <td>asian shares edge up after wall st gains but c...</td>\n",
       "      <td>الأسهم الأسيوية ترتفع بعد المكاسب التي حققتها ...</td>\n",
       "      <td>Asian stocks rise after Wall Street gains, but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text  \\\n",
       "0  advertising  asks sec to mull halfyear corporate filings vs...   \n",
       "1  advertising  st up on trade hopes sp equals longest bull ru...   \n",
       "2  advertising  shares hit oneyear low on turkey china worries...   \n",
       "3  advertising  stocks weaken as turkey worries weigh dollar s...   \n",
       "4  advertising  asian shares edge up after wall st gains but c...   \n",
       "\n",
       "                                              Arabic  \\\n",
       "0  يسأل ثانية للنظر في طلبات الشركات نصف السنة مق...   \n",
       "1  التباطؤ في الآمال التجارية: sp تساوي أطول فترة...   \n",
       "2  الأسهم الصينية تتراجع إلى أدنى مستوياتها مع ال...   \n",
       "3  الأسهم الأسيوية تضعف مع قلق تركيا من ارتفاع ال...   \n",
       "4  الأسهم الأسيوية ترتفع بعد المكاسب التي حققتها ...   \n",
       "\n",
       "                                          NewEnglish  \n",
       "0  Asked again to look at half-year corporate req...  \n",
       "1  Deceleration in business hopes: sp equals the ...  \n",
       "2  Chinese stocks fall to their lowest levels wit...  \n",
       "3  Asian stocks weaken as Turkey worries about do...  \n",
       "4  Asian stocks rise after Wall Street gains, but...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RT = pd.read_csv(\"mydatasetWitharabicEnglishText.csv\")\n",
    "RT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the first part from the CSV to have the same training and test split as the previous classifier, for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advertising</td>\n",
       "      <td>asks sec to mull halfyear corporate filings vs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advertising</td>\n",
       "      <td>stocks weaken as turkey worries weigh dollar s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advertising</td>\n",
       "      <td>asian shares edge up after wall st gains but c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advertising</td>\n",
       "      <td>bid for tesla no formal offer no firm deals wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>advertising</td>\n",
       "      <td>st rallies on solid earnings uschina trade tal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "0  advertising  asks sec to mull halfyear corporate filings vs...\n",
       "1  advertising  stocks weaken as turkey worries weigh dollar s...\n",
       "2  advertising  asian shares edge up after wall st gains but c...\n",
       "3  advertising  bid for tesla no formal offer no firm deals wi...\n",
       "4  advertising  st rallies on solid earnings uschina trade tal..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Part1 = pd.read_csv(\"englishtrain.csv\",header=None)\n",
    "Part1.columns = [\"label\",\"text\"]\n",
    "Part1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the second part of the dataset from the translated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advertising</td>\n",
       "      <td>Asked again to look at half-year corporate req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advertising</td>\n",
       "      <td>Deceleration in business hopes: sp equals the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advertising</td>\n",
       "      <td>Chinese stocks fall to their lowest levels wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advertising</td>\n",
       "      <td>Asian stocks weaken as Turkey worries about do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>advertising</td>\n",
       "      <td>Asian stocks rise after Wall Street gains, but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "0  advertising  Asked again to look at half-year corporate req...\n",
       "1  advertising  Deceleration in business hopes: sp equals the ...\n",
       "2  advertising  Chinese stocks fall to their lowest levels wit...\n",
       "3  advertising  Asian stocks weaken as Turkey worries about do...\n",
       "4  advertising  Asian stocks rise after Wall Street gains, but..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Part2 = RT[\"label\"]\n",
    "Part2 = pd.DataFrame(Part2)\n",
    "Part2[\"text\"] = RT[\"NewEnglish\"]\n",
    "Part2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the to parts to have the original training part of english dataset, alongwith the arabic dataset that have got translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_Translated = Part1.append(Part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The path to the test dataset is provided to the model, in order to have the same validation dataset as the previous model to have a fair comparison. So the validation dataset is totally separated from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4593it [00:01, 3790.21it/s]\n",
      "925it [00:00, 5359.65it/s]\n",
      "100%|██████████| 5518/5518 [00:00<00:00, 13031.99it/s]\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (hidden2tag): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0583, loss: 3.7051 ||: 100%|██████████| 144/144 [00:32<00:00,  4.77it/s]\n",
      "accuracy: 0.0692, loss: 3.7097 ||: 100%|██████████| 29/29 [00:01<00:00, 20.89it/s]\n",
      "accuracy: 0.1034, loss: 3.3974 ||: 100%|██████████| 144/144 [00:30<00:00,  5.27it/s]\n",
      "accuracy: 0.0778, loss: 3.6678 ||: 100%|██████████| 29/29 [00:01<00:00, 23.35it/s]\n",
      "accuracy: 0.1380, loss: 3.0907 ||: 100%|██████████| 144/144 [00:30<00:00,  4.45it/s]\n",
      "accuracy: 0.1243, loss: 3.4553 ||: 100%|██████████| 29/29 [00:01<00:00, 22.91it/s]\n",
      "accuracy: 0.2197, loss: 2.7675 ||: 100%|██████████| 144/144 [00:30<00:00,  4.46it/s]\n",
      "accuracy: 0.1286, loss: 3.3195 ||: 100%|██████████| 29/29 [00:01<00:00, 19.64it/s]\n",
      "accuracy: 0.3055, loss: 2.4183 ||: 100%|██████████| 144/144 [00:30<00:00,  4.79it/s]\n",
      "accuracy: 0.1665, loss: 3.2709 ||: 100%|██████████| 29/29 [00:01<00:00, 22.87it/s]\n",
      "accuracy: 0.4156, loss: 2.0200 ||: 100%|██████████| 144/144 [00:29<00:00,  4.76it/s]\n",
      "accuracy: 0.2151, loss: 3.2400 ||: 100%|██████████| 29/29 [00:01<00:00, 23.24it/s]\n",
      "accuracy: 0.5354, loss: 1.6327 ||: 100%|██████████| 144/144 [00:29<00:00,  4.60it/s]\n",
      "accuracy: 0.2314, loss: 3.2786 ||: 100%|██████████| 29/29 [00:01<00:00, 23.25it/s]\n",
      "accuracy: 0.6231, loss: 1.2788 ||: 100%|██████████| 144/144 [00:30<00:00,  5.00it/s]\n",
      "accuracy: 0.2411, loss: 3.4610 ||: 100%|██████████| 29/29 [00:01<00:00, 22.32it/s]\n",
      "accuracy: 0.7100, loss: 1.0211 ||: 100%|██████████| 144/144 [00:33<00:00,  4.04it/s]\n",
      "accuracy: 0.2314, loss: 3.5664 ||: 100%|██████████| 29/29 [00:01<00:00, 17.25it/s]\n",
      "accuracy: 0.7546, loss: 0.8272 ||: 100%|██████████| 144/144 [00:32<00:00,  5.05it/s]\n",
      "accuracy: 0.2357, loss: 3.7642 ||: 100%|██████████| 29/29 [00:01<00:00, 25.93it/s]\n",
      "accuracy: 0.8141, loss: 0.6397 ||: 100%|██████████| 144/144 [00:35<00:00,  3.95it/s]\n",
      "accuracy: 0.2389, loss: 3.8359 ||: 100%|██████████| 29/29 [00:01<00:00, 21.34it/s]\n",
      "accuracy: 0.8402, loss: 0.5072 ||: 100%|██████████| 144/144 [00:36<00:00,  3.83it/s]\n",
      "accuracy: 0.2346, loss: 4.0030 ||: 100%|██████████| 29/29 [00:01<00:00, 17.82it/s]\n",
      "accuracy: 0.8661, loss: 0.4118 ||: 100%|██████████| 144/144 [00:39<00:00,  3.82it/s]\n",
      "accuracy: 0.2422, loss: 4.1132 ||: 100%|██████████| 29/29 [00:01<00:00, 17.45it/s]\n",
      "accuracy: 0.8700, loss: 0.4176 ||: 100%|██████████| 144/144 [00:37<00:00,  3.59it/s]\n",
      "accuracy: 0.2368, loss: 4.0559 ||: 100%|██████████| 29/29 [00:01<00:00, 16.77it/s]\n",
      "accuracy: 0.8689, loss: 0.3884 ||: 100%|██████████| 144/144 [00:37<00:00,  3.39it/s]\n",
      "accuracy: 0.2486, loss: 4.2542 ||: 100%|██████████| 29/29 [00:01<00:00, 16.82it/s]\n",
      "accuracy: 0.8870, loss: 0.3212 ||: 100%|██████████| 144/144 [00:41<00:00,  2.83it/s]\n",
      "accuracy: 0.2378, loss: 4.4782 ||: 100%|██████████| 29/29 [00:02<00:00, 13.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_epoch': 5, 'peak_cpu_memory_MB': 1461.52, 'training_duration': '00:08:37', 'training_start_epoch': 0, 'training_epochs': 14, 'epoch': 14, 'training_accuracy': 0.8689309819290224, 'training_loss': 0.38841870703941417, 'training_cpu_memory_MB': 1461.52, 'validation_accuracy': 0.24864864864864866, 'validation_loss': 4.254193289526578, 'best_validation_accuracy': 0.21513513513513513, 'best_validation_loss': 3.240029030832751}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testPath = 'englishtest.csv'\n",
    "trainAllLang(RT_Translated,\"RT_Translated\",testPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
